# ðŸŒŒ Gradient Echoes
> *Classical grit, quantum wit.*  
Optimization algorithms â€” both **classical** (SGD, Adam, L-BFGS, etc.) and **quantum-inspired** (QNG, SPSA, QAOA) â€” unified in one playground.

![Optimizer Comparison](docs/assets/loss_curves.png)

---

## âœ¨ Features
- ðŸ“¦ Minimal, educational implementations of:
  - **Classical**: SGD (with momentum/Nesterov), Adam/AMSGrad, RMSProp, AdaGrad, L-BFGS, Nelderâ€“Mead, SPSA
  - **Quantum**: Quantum Natural Gradient, QAOA loop, Parameter-Shift rule, Quantum SPSA
- ðŸ§® Clean abstractions: `Objective`, `Oracle`, `Constraint`, `Callback`, `Schedule`
- ðŸ“ˆ Visualizations: loss curves, 3D surfaces, optimizer trajectories
- ðŸ§ª Fully tested with `pytest`, pre-commit linting, CI

---

## ðŸš€ Quickstart

Install locally:
```bash
git clone https://github.com/<you>/gradient-echoes.git
cd gradient-echoes
pip install -e ".[dev]"
